{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prjmod as pm\n",
    "import prjmod.commons as commons\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data \n",
    "\n",
    "## Local file to Spark DataFrame (SDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, FloatType\n",
    "\n",
    "csv_schema = StructType([ \\\n",
    "    StructField(\"_c0\",StringType(),True), \\\n",
    "    StructField(\"year\",IntegerType(),True), \\\n",
    "    StructField(\"month\",IntegerType(),True), \\\n",
    "    StructField(\"day\", IntegerType(), True), \\\n",
    "    StructField(\"dep_time\", FloatType(), True), \\\n",
    "    StructField(\"sched_dep_time\", IntegerType(), True), \\\n",
    "    StructField(\"dep_delay\",FloatType(),True), \\\n",
    "    StructField(\"arr_time\",FloatType(),True), \\\n",
    "    StructField(\"sched_arr_time\",IntegerType(),True), \\\n",
    "    StructField(\"arr_delay\", FloatType(), True), \\\n",
    "    StructField(\"carrier\", StringType(), True), \\\n",
    "    StructField(\"flight\", StringType(), True), \\\n",
    "    StructField(\"tailnum\", StringType(), True), \\\n",
    "    StructField(\"origin\", StringType(), True), \\\n",
    "    StructField(\"dest\",StringType(),True), \\\n",
    "    StructField(\"air_time\",FloatType(),True), \\\n",
    "    StructField(\"distance\",IntegerType(),True), \\\n",
    "    StructField(\"hour\", IntegerType(), True), \\\n",
    "    StructField(\"minute\", IntegerType(), True), \\\n",
    "    StructField(\"time_hour\", TimestampType(), True) \\\n",
    "  ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dep_time',\n",
       " 'sched_dep_time',\n",
       " 'dep_delay',\n",
       " 'arr_time',\n",
       " 'sched_arr_time',\n",
       " 'arr_delay',\n",
       " 'carrier',\n",
       " 'flight',\n",
       " 'tailnum',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'air_time',\n",
       " 'distance',\n",
       " 'hour',\n",
       " 'minute',\n",
       " 'time_hour']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sdf_flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_flights = spark.read.csv(commons.DL_FILE_FLIGHTS, header=True, sep = ';', schema = csv_schema)\n",
    "# sdf_flights = spark.read.csv(commons.DL_FILE_FLIGHTS, header=True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: float (nullable = true)\n",
      " |-- sched_dep_time: integer (nullable = true)\n",
      " |-- dep_delay: float (nullable = true)\n",
      " |-- arr_time: float (nullable = true)\n",
      " |-- sched_arr_time: integer (nullable = true)\n",
      " |-- arr_delay: float (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: float (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- time_hour: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|_c0|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|  1|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|\n",
      "|  2|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|\n",
      "|  3|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|\n",
      "|  4|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|\n",
      "|  5|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|\n",
      "|  6|2013|    1|  1|   554.0|           558|     -4.0|   740.0|           728|     12.0|     UA|  1696| N39463|   EWR| ORD|   150.0|     719|   5|    58|2013-01-01 05:00:00|\n",
      "|  7|2013|    1|  1|   555.0|           600|     -5.0|   913.0|           854|     19.0|     B6|   507| N516JB|   EWR| FLL|   158.0|    1065|   6|     0|2013-01-01 06:00:00|\n",
      "|  8|2013|    1|  1|   557.0|           600|     -3.0|   709.0|           723|    -14.0|     EV|  5708| N829AS|   LGA| IAD|    53.0|     229|   6|     0|2013-01-01 06:00:00|\n",
      "|  9|2013|    1|  1|   557.0|           600|     -3.0|   838.0|           846|     -8.0|     B6|    79| N593JB|   JFK| MCO|   140.0|     944|   6|     0|2013-01-01 06:00:00|\n",
      "| 10|2013|    1|  1|   558.0|           600|     -2.0|   753.0|           745|      8.0|     AA|   301| N3ALAA|   LGA| ORD|   138.0|     733|   6|     0|2013-01-01 06:00:00|\n",
      "| 11|2013|    1|  1|   558.0|           600|     -2.0|   849.0|           851|     -2.0|     B6|    49| N793JB|   JFK| PBI|   149.0|    1028|   6|     0|2013-01-01 06:00:00|\n",
      "| 12|2013|    1|  1|   558.0|           600|     -2.0|   853.0|           856|     -3.0|     B6|    71| N657JB|   JFK| TPA|   158.0|    1005|   6|     0|2013-01-01 06:00:00|\n",
      "| 13|2013|    1|  1|   558.0|           600|     -2.0|   924.0|           917|      7.0|     UA|   194| N29129|   JFK| LAX|   345.0|    2475|   6|     0|2013-01-01 06:00:00|\n",
      "| 14|2013|    1|  1|   558.0|           600|     -2.0|   923.0|           937|    -14.0|     UA|  1124| N53441|   EWR| SFO|   361.0|    2565|   6|     0|2013-01-01 06:00:00|\n",
      "| 15|2013|    1|  1|   559.0|           600|     -1.0|   941.0|           910|     31.0|     AA|   707| N3DUAA|   LGA| DFW|   257.0|    1389|   6|     0|2013-01-01 06:00:00|\n",
      "| 16|2013|    1|  1|   559.0|           559|      0.0|   702.0|           706|     -4.0|     B6|  1806| N708JB|   JFK| BOS|    44.0|     187|   5|    59|2013-01-01 05:00:00|\n",
      "| 17|2013|    1|  1|   559.0|           600|     -1.0|   854.0|           902|     -8.0|     UA|  1187| N76515|   EWR| LAS|   337.0|    2227|   6|     0|2013-01-01 06:00:00|\n",
      "| 18|2013|    1|  1|   600.0|           600|      0.0|   851.0|           858|     -7.0|     B6|   371| N595JB|   LGA| FLL|   152.0|    1076|   6|     0|2013-01-01 06:00:00|\n",
      "| 19|2013|    1|  1|   600.0|           600|      0.0|   837.0|           825|     12.0|     MQ|  4650| N542MQ|   LGA| ATL|   134.0|     762|   6|     0|2013-01-01 06:00:00|\n",
      "| 20|2013|    1|  1|   601.0|           600|      1.0|   844.0|           850|     -6.0|     B6|   343| N644JB|   EWR| PBI|   147.0|    1023|   6|     0|2013-01-01 06:00:00|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, _c0: string, year: string, month: string, day: string, dep_time: string, sched_dep_time: string, dep_delay: string, arr_time: string, sched_arr_time: string, arr_delay: string, carrier: string, flight: string, tailnum: string, origin: string, dest: string, air_time: string, distance: string, hour: string, minute: string]\n"
     ]
    }
   ],
   "source": [
    "print(sdf_flights.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='sdf_flights_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Add sdf_flights to the catalog\n",
    "sdf_flights.createOrReplaceTempView(\"sdf_flights_temp\")\n",
    "\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get the data as SDF from the table in the Spark cluster:\n",
    "sdf_flights_frm_table = spark.table('sdf_flights_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDF to pandas DataFrame (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_flights = sdf_flights.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _c0  year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n",
      "0   1  2013      1    1     517.0             515        2.0     830.0   \n",
      "1   2  2013      1    1     533.0             529        4.0     850.0   \n",
      "2   3  2013      1    1     542.0             540        2.0     923.0   \n",
      "3   4  2013      1    1     544.0             545       -1.0    1004.0   \n",
      "4   5  2013      1    1     554.0             600       -6.0     812.0   \n",
      "\n",
      "   sched_arr_time  arr_delay carrier flight tailnum origin dest  air_time  \\\n",
      "0             819       11.0      UA   1545  N14228    EWR  IAH     227.0   \n",
      "1             830       20.0      UA   1714  N24211    LGA  IAH     227.0   \n",
      "2             850       33.0      AA   1141  N619AA    JFK  MIA     160.0   \n",
      "3            1022      -18.0      B6    725  N804JB    JFK  BQN     183.0   \n",
      "4             837      -25.0      DL    461  N668DN    LGA  ATL     116.0   \n",
      "\n",
      "   distance  hour  minute           time_hour  \n",
      "0      1400     5      15 2013-01-01 05:00:00  \n",
      "1      1416     5      29 2013-01-01 05:00:00  \n",
      "2      1089     5      40 2013-01-01 05:00:00  \n",
      "3      1576     5      45 2013-01-01 05:00:00  \n",
      "4       762     6       0 2013-01-01 06:00:00  \n"
     ]
    }
   ],
   "source": [
    "print(pdf_flights.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 336776 entries, 0 to 336775\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   _c0             336776 non-null  object        \n",
      " 1   year            336776 non-null  int32         \n",
      " 2   month           336776 non-null  int32         \n",
      " 3   day             336776 non-null  int32         \n",
      " 4   dep_time        328521 non-null  float32       \n",
      " 5   sched_dep_time  336776 non-null  int32         \n",
      " 6   dep_delay       328521 non-null  float32       \n",
      " 7   arr_time        328063 non-null  float32       \n",
      " 8   sched_arr_time  336776 non-null  int32         \n",
      " 9   arr_delay       327346 non-null  float32       \n",
      " 10  carrier         336776 non-null  object        \n",
      " 11  flight          336776 non-null  object        \n",
      " 12  tailnum         336776 non-null  object        \n",
      " 13  origin          336776 non-null  object        \n",
      " 14  dest            336776 non-null  object        \n",
      " 15  air_time        327346 non-null  float32       \n",
      " 16  distance        336776 non-null  int32         \n",
      " 17  hour            336776 non-null  int32         \n",
      " 18  minute          336776 non-null  int32         \n",
      " 19  time_hour       336776 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float32(5), int32(8), object(6)\n",
      "memory usage: 34.7+ MB\n"
     ]
    }
   ],
   "source": [
    "pdf_flights.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas to SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sdf_flights_2 from pd_temp\n",
    "sdf_flights_2 = spark.createDataFrame(pdf_flights.iloc[0:100, :], schema=csv_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o259.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 85) (ESW5CG8461BGN.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25008\\2545683905.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msdf_flights_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\trainSpark\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\trainSpark\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\trainSpark\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\trainSpark\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o259.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 85) (ESW5CG8461BGN.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "sdf_flights_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='sdf_flights_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='sdf_flights_2_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='sdf_flights_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add sdf_flights_2 to the catalog\n",
    "sdf_flights_2.createOrReplaceTempView(\"sdf_flights_2_temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Spark Tables with SQL Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string, year: int, month: int, day: int, dep_time: float, sched_dep_time: int, dep_delay: float, arr_time: float, sched_arr_time: int, arr_delay: float, carrier: string, flight: string, tailnum: string, origin: string, dest: string, air_time: float, distance: int, hour: int, minute: int, time_hour: timestamp]\n"
     ]
    }
   ],
   "source": [
    "flights10 = spark.sql(\"SELECT * FROM sdf_flights_temp LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|_c0|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|  1|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|\n",
      "|  2|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|\n",
      "|  3|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|\n",
      "|  4|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|\n",
      "|  5|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|\n",
      "|  6|2013|    1|  1|   554.0|           558|     -4.0|   740.0|           728|     12.0|     UA|  1696| N39463|   EWR| ORD|   150.0|     719|   5|    58|2013-01-01 05:00:00|\n",
      "|  7|2013|    1|  1|   555.0|           600|     -5.0|   913.0|           854|     19.0|     B6|   507| N516JB|   EWR| FLL|   158.0|    1065|   6|     0|2013-01-01 06:00:00|\n",
      "|  8|2013|    1|  1|   557.0|           600|     -3.0|   709.0|           723|    -14.0|     EV|  5708| N829AS|   LGA| IAD|    53.0|     229|   6|     0|2013-01-01 06:00:00|\n",
      "|  9|2013|    1|  1|   557.0|           600|     -3.0|   838.0|           846|     -8.0|     B6|    79| N593JB|   JFK| MCO|   140.0|     944|   6|     0|2013-01-01 06:00:00|\n",
      "| 10|2013|    1|  1|   558.0|           600|     -2.0|   753.0|           745|      8.0|     AA|   301| N3ALAA|   LGA| ORD|   138.0|     733|   6|     0|2013-01-01 06:00:00|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|_c0|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|  1|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|\n",
      "|  2|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|\n",
      "|  3|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|\n",
      "|  4|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|\n",
      "|  7|2013|    1|  1|   555.0|           600|     -5.0|   913.0|           854|     19.0|     B6|   507| N516JB|   EWR| FLL|   158.0|    1065|   6|     0|2013-01-01 06:00:00|\n",
      "| 11|2013|    1|  1|   558.0|           600|     -2.0|   849.0|           851|     -2.0|     B6|    49| N793JB|   JFK| PBI|   149.0|    1028|   6|     0|2013-01-01 06:00:00|\n",
      "| 12|2013|    1|  1|   558.0|           600|     -2.0|   853.0|           856|     -3.0|     B6|    71| N657JB|   JFK| TPA|   158.0|    1005|   6|     0|2013-01-01 06:00:00|\n",
      "| 13|2013|    1|  1|   558.0|           600|     -2.0|   924.0|           917|      7.0|     UA|   194| N29129|   JFK| LAX|   345.0|    2475|   6|     0|2013-01-01 06:00:00|\n",
      "| 14|2013|    1|  1|   558.0|           600|     -2.0|   923.0|           937|    -14.0|     UA|  1124| N53441|   EWR| SFO|   361.0|    2565|   6|     0|2013-01-01 06:00:00|\n",
      "| 15|2013|    1|  1|   559.0|           600|     -1.0|   941.0|           910|     31.0|     AA|   707| N3DUAA|   LGA| DFW|   257.0|    1389|   6|     0|2013-01-01 06:00:00|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter flights by passing a string\n",
    "long_flights1 = sdf_flights.filter(\"distance > 1000\")\n",
    "\n",
    "# Filter flights by passing a column of boolean values\n",
    "long_flights2 = sdf_flights.filter(sdf_flights.distance > 1000)\n",
    "\n",
    "# Print the data to check they're equal\n",
    "long_flights1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|_c0|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "|  1|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|\n",
      "|  2|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|\n",
      "|  3|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|\n",
      "|  4|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|\n",
      "|  7|2013|    1|  1|   555.0|           600|     -5.0|   913.0|           854|     19.0|     B6|   507| N516JB|   EWR| FLL|   158.0|    1065|   6|     0|2013-01-01 06:00:00|\n",
      "| 11|2013|    1|  1|   558.0|           600|     -2.0|   849.0|           851|     -2.0|     B6|    49| N793JB|   JFK| PBI|   149.0|    1028|   6|     0|2013-01-01 06:00:00|\n",
      "| 12|2013|    1|  1|   558.0|           600|     -2.0|   853.0|           856|     -3.0|     B6|    71| N657JB|   JFK| TPA|   158.0|    1005|   6|     0|2013-01-01 06:00:00|\n",
      "| 13|2013|    1|  1|   558.0|           600|     -2.0|   924.0|           917|      7.0|     UA|   194| N29129|   JFK| LAX|   345.0|    2475|   6|     0|2013-01-01 06:00:00|\n",
      "| 14|2013|    1|  1|   558.0|           600|     -2.0|   923.0|           937|    -14.0|     UA|  1124| N53441|   EWR| SFO|   361.0|    2565|   6|     0|2013-01-01 06:00:00|\n",
      "| 15|2013|    1|  1|   559.0|           600|     -1.0|   941.0|           910|     31.0|     AA|   707| N3DUAA|   LGA| DFW|   257.0|    1389|   6|     0|2013-01-01 06:00:00|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_flights2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = sdf_flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "\n",
    "# Select the second set of columns\n",
    "temp = sdf_flights.select(sdf_flights.origin, sdf_flights.dest, sdf_flights.carrier)\n",
    "\n",
    "# Define first filter\n",
    "filterA = sdf_flights.origin == \"SEA\"\n",
    "\n",
    "# Define second filter\n",
    "filterB = sdf_flights.dest == \"PDX\"\n",
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (sdf_flights.distance/(sdf_flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "speed1 = sdf_flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = sdf_flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|           94|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(air_time)|\n",
      "+-------------+\n",
      "|        691.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "sdf_flights.filter(sdf_flights.origin == 'JFK').groupBy().min('distance').show()\n",
    "\n",
    "# Find the longest flight from SEA in terms of air time\n",
    "sdf_flights.filter(sdf_flights.origin == 'JFK').groupBy().max('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(air_time)|\n",
      "+------------------+\n",
      "|229.81132350795272|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|sum(duration_hrs)|\n",
      "+-----------------+\n",
      "|822110.1666666731|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average duration of Delta flights\n",
    "sdf_flights.filter(sdf_flights.carrier == \"DL\").filter(sdf_flights.origin == \"JFK\").groupBy().avg(\"air_time\").show()\n",
    "\n",
    "# Total hours in the air\n",
    "sdf_flights.withColumn(\"duration_hrs\", sdf_flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|tailnum|count|\n",
      "+-------+-----+\n",
      "| N513UA|  102|\n",
      "| N510UW|   48|\n",
      "| N8390A|   31|\n",
      "| N3CWAA|   68|\n",
      "| N73283|  110|\n",
      "| N369NB|  187|\n",
      "| N396AA|   21|\n",
      "| N8322X|   15|\n",
      "| N3AEMQ|  276|\n",
      "| N4YUAA|   42|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by tailnum\n",
    "by_plane = sdf_flights.groupBy(\"tailnum\")\n",
    "\n",
    "# Number of flights each plane made\n",
    "by_plane.count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|     avg(air_time)|\n",
      "+------+------------------+\n",
      "|   LGA|117.82580581372355|\n",
      "|   EWR|153.30002475944914|\n",
      "|   JFK| 178.3490497712667|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by origin\n",
    "by_origin = sdf_flights.groupBy(\"origin\")\n",
    "\n",
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+\n",
      "|month|dest|     avg(dep_delay)|\n",
      "+-----+----+-------------------+\n",
      "|    1| EYW|               13.0|\n",
      "|   10| CLE|  4.405172413793103|\n",
      "|   10| JAX|  9.313807531380753|\n",
      "|   10| BHM| 24.153846153846153|\n",
      "|   10| DAY| 15.118110236220472|\n",
      "|   11| OKC|   8.10344827586207|\n",
      "|   10| DCA|  4.209424083769633|\n",
      "|   10| DFW|  3.522948539638387|\n",
      "|   10| OMA|  13.39080459770115|\n",
      "|   10| MHT|  13.80722891566265|\n",
      "|   11| LAS|  4.782700421940929|\n",
      "|    1| MSP|  11.76172607879925|\n",
      "|   10| BTV| 2.5508474576271185|\n",
      "|   10| MEM|  7.928104575163399|\n",
      "|   11| BHM|  19.61904761904762|\n",
      "|   12| MEM| 31.747899159663866|\n",
      "|   12| ILM|              31.25|\n",
      "|   11| HNL|0.18181818181818182|\n",
      "|    1| SLC|  8.360406091370558|\n",
      "|   10| TUL| 30.153846153846153|\n",
      "+-----+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = sdf_flights.groupBy('month', 'dest')\n",
    "\n",
    "# Average departure delay by month and destination\n",
    "by_month_dest.agg(F.avg('dep_delay')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------------------+\n",
      "|month|dest|stddev_samp(dep_delay)|\n",
      "+-----+----+----------------------+\n",
      "|    1| EYW|                  null|\n",
      "|   10| CLE|    25.820811853116663|\n",
      "|   10| JAX|    33.089734402632345|\n",
      "|   10| BHM|     46.98058518809003|\n",
      "|   10| DAY|     38.15998364721879|\n",
      "|   11| OKC|     19.23416756635034|\n",
      "|   10| DCA|    29.070946167440905|\n",
      "|   10| DFW|     25.93755548637475|\n",
      "|   10| OMA|     37.85663172581741|\n",
      "|   10| MHT|    31.988358336380983|\n",
      "|   11| LAS|    26.271577988930474|\n",
      "|    1| MSP|     42.05931944173929|\n",
      "|   10| BTV|     20.65917856191837|\n",
      "|   10| MEM|      26.8930886678019|\n",
      "|   11| BHM|    26.433834739734962|\n",
      "|   12| MEM|      52.2012433268701|\n",
      "|   12| ILM|      54.0333401878123|\n",
      "|   11| HNL|    15.146312237992932|\n",
      "|    1| SLC|     33.21385646170493|\n",
      "|   10| TUL|     43.20341866814922|\n",
      "+-----+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard deviation of departure delay\n",
    "by_month_dest.agg(F.stddev('dep_delay')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7d1805cf74aaf3c0d16065b69b4acce1ce5d0e02a23802bb00e8000da4a34d5"
  },
  "kernelspec": {
   "display_name": "trainSpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
